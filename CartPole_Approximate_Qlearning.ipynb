{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpALhA5Ome6IuFKqAjy2kk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ogunfool/Approximate-RL-Methods-for-CartPole-Environment-on-OpenAI-gym/blob/main/CartPole_Approximate_Qlearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUVzrE0uIK8n"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.kernel_approximation import RBFSampler\n",
        "\n",
        "# Global Variables\n",
        "GAMMA = 0.99\n",
        "ALPHA = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Some checks\n",
        "# OpenAI - gym\n",
        "import gym\n",
        "env = gym.make('CartPole-v1')\n",
        "s = env.reset()\n",
        "a = env.action_space.sample()\n",
        "print(s)\n",
        "print(a)\n",
        "\n",
        "env.action_space.n"
      ],
      "metadata": {
        "id": "GaeFmCydhCCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets build some functions/callbacks\n",
        "# Epsilon_greedy function (Behaviour Policy)\n",
        "def epsilon_greedy(model, s, eps=0.1):\n",
        "  # we'll use epsilon-soft to ensure all states are visited\n",
        "  # what happens if you don't do this? i.e. eps=0\n",
        "  p = np.random.random()\n",
        "  if p < (1 - eps):\n",
        "    values = model.predict_all_actions(s)\n",
        "    return np.argmax(values)\n",
        "  else:\n",
        "    return model.env.action_space.sample() #This is how you generate random actions from the environment (OpenAI gym)\n",
        "\n",
        "\n",
        "def gather_samples(env, n_episodes=10000):\n",
        "  samples = []\n",
        "  for _ in range(n_episodes):\n",
        "    s = env.reset()\n",
        "    done = False\n",
        "    truncated = False\n",
        "    while not (done or truncated):\n",
        "      a = env.action_space.sample()\n",
        "      sa = np.concatenate((s, [a]))\n",
        "      samples.append(sa)\n",
        "\n",
        "      s, r, done, truncated = env.step(a)\n",
        "  return samples"
      ],
      "metadata": {
        "id": "WKapZcvbdn3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets build the model\n",
        "# env.action_space.n = returns the no of elements in the action space method of the environment\n",
        "class Model:\n",
        "  def __init__(self, env):\n",
        "    # fit the featurizer to data\n",
        "    self.env = env\n",
        "    samples = gather_samples(env)\n",
        "    self.featurizer = RBFSampler()\n",
        "    self.featurizer.fit(samples)\n",
        "    dims = self.featurizer.n_components # returns the dimension or the no of components the featurizer will return\n",
        "\n",
        "    # initialize linear model weights\n",
        "    self.w = np.zeros(dims)\n",
        "\n",
        "  def predict(self, s, a):\n",
        "    sa = np.concatenate((s, [a]))\n",
        "    x = self.featurizer.transform([sa])[0]\n",
        "    return x @ self.w\n",
        "\n",
        "  def predict_all_actions(self, s):\n",
        "    return [self.predict(s, a) for a in range(self.env.action_space.n)]\n",
        "\n",
        "  def grad(self, s, a):\n",
        "    sa = np.concatenate((s, [a]))\n",
        "    x = self.featurizer.transform([sa])[0]\n",
        "    return x\n",
        "\n",
        "\n",
        "def test_agent(model, env, n_episodes=20):\n",
        "  reward_per_episode = np.zeros(n_episodes)\n",
        "  for it in range(n_episodes):\n",
        "    done = False\n",
        "    truncated = False\n",
        "    episode_reward = 0\n",
        "    s = env.reset()\n",
        "    while not (done or truncated):\n",
        "      a = epsilon_greedy(model, s, eps=0) # epsilon = 0 during test. i.e Always act greedily, no exploration. # But not randomly, based on the Q values\n",
        "      s, r, done, truncated = env.step(a)\n",
        "      episode_reward += r\n",
        "    reward_per_episode[it] = episode_reward\n",
        "  return np.mean(reward_per_episode)\n",
        "\n",
        "# Play one episode, you basically just want to see how many rewards you can collect/episode in a single episode\n",
        "def watch_agent(model, env, eps):\n",
        "  done = False\n",
        "  truncated = False\n",
        "  episode_reward = 0\n",
        "  s = env.reset()\n",
        "  while not (done or truncated):\n",
        "    a = epsilon_greedy(model, s, eps=eps)\n",
        "    s, r, done, truncated = env.step(a)\n",
        "    episode_reward += r\n",
        "  print(\"Episode reward:\", episode_reward)\n"
      ],
      "metadata": {
        "id": "Od49PjFLd_H2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QS1lvLYKBadL"
      },
      "outputs": [],
      "source": [
        "# instantiate environment\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "model = Model(env)\n",
        "reward_per_episode = []\n",
        "\n",
        "# watch untrained agent\n",
        "watch_agent(model, env, eps=0)\n",
        "\n",
        "# Basic Q-learning loop (s,a,r,s2)\n",
        "# repeat until convergence\n",
        "n_episodes = 1500\n",
        "for it in range(n_episodes):\n",
        "  s = env.reset()\n",
        "  episode_reward = 0\n",
        "  done = False\n",
        "  truncated = False\n",
        "  while not (done or truncated):\n",
        "    a = epsilon_greedy(model, s)\n",
        "    s2, r, done, truncated = env.step(a)\n",
        "\n",
        "    # Let's do gradient descent update\n",
        "    # get the target\n",
        "    if done:\n",
        "      target = r\n",
        "    else:\n",
        "      values = model.predict_all_actions(s2) # A prediction over all possible actions for a given state\n",
        "      target = r + GAMMA * np.max(values)\n",
        "\n",
        "    # update the model\n",
        "    g = model.grad(s, a)\n",
        "    err = target - model.predict(s, a)\n",
        "    model.w += ALPHA * err * g\n",
        "    \n",
        "    # accumulate reward\n",
        "    episode_reward += r\n",
        "\n",
        "    # update state\n",
        "    s = s2\n",
        "\n",
        "  if (it + 1) % 50 == 0:\n",
        "    print(f\"Episode: {it + 1}, Reward: {episode_reward}\")\n",
        "\n",
        "  # early exit\n",
        "  if it > 20 and np.mean(reward_per_episode[-20:]) == 200:\n",
        "    print(\"Early exit\")\n",
        "    break\n",
        "  \n",
        "  reward_per_episode.append(episode_reward)\n",
        "\n",
        "# test trained agent\n",
        "test_reward = test_agent(model, env)\n",
        "print(f\"Average test reward: {test_reward}\")\n",
        "\n",
        "plt.plot(reward_per_episode)\n",
        "plt.title(\"Reward per episode\")\n",
        "plt.show()\n",
        "\n",
        "# watch trained agent\n",
        "watch_agent(model, env, eps=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " watch_agent(model, env, eps=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhUFpVkgOiKN",
        "outputId": "a67daf6f-c0b1-49b2-d379-d2c764d9f0e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode reward: 65.0\n"
          ]
        }
      ]
    }
  ]
}